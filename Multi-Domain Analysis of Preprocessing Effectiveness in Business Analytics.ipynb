{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "381b5458-d903-4e1a-933a-7e003e877c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 12:57:45 - preprocessing_study - INFO - Study initialized at 2025-09-03 12:57:45.459588\n",
      "2025-09-03 12:57:45 - preprocessing_study - INFO - Configuration: StudyConfiguration(random_state=42, test_size=0.25, n_iterations=10, cv_folds=5, significance_level=0.05, minimal_effect=0.005, small_effect=0.015, medium_effect=0.025, large_effect=0.035, high_quality_missing=0.02, medium_quality_missing=0.1, low_quality_missing=0.25)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCI ML Repository available\n",
      "=== MULTI-DOMAIN PREPROCESSING EFFECTIVENESS STUDY ===\n",
      "Timestamp: 2025-09-03 12:57:45.460205\n",
      "Random seed: 42\n",
      "Effect size thresholds: 0.005 (minimal) to 0.035 (large)\n"
     ]
    }
   ],
   "source": [
    "# Multi-Domain Analysis of Preprocessing Effectiveness in Business Analytics\n",
    "# A Benchmark Study of Data Characteristics and Performance Outcomes\n",
    "# \n",
    "# Authors: [Your Name]\n",
    "# Affiliation: [Your Institution]\n",
    "# Target Journal: Journal of Business Analytics\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional, Any, Callable, Protocol\n",
    "from dataclasses import dataclass\n",
    "from abc import ABC, abstractmethod\n",
    "import json\n",
    "import gc\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import NamedTuple, Set\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from sklearn.utils import resample\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# UCI ML Repository for dataset loading\n",
    "try:\n",
    "    from ucimlrepo import fetch_ucirepo\n",
    "    UCI_AVAILABLE = True\n",
    "    print(\"UCI ML Repository available\")\n",
    "except ImportError:\n",
    "    UCI_AVAILABLE = False\n",
    "    print(\"WARNING: ucimlrepo not available. Install with: pip install ucimlrepo\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 1: Configuration and Logging Setup\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class StudyConfiguration:\n",
    "    \"\"\"Single Responsibility: Configuration management for the entire study\"\"\"\n",
    "    random_state: int = 42\n",
    "    test_size: float = 0.25\n",
    "    n_iterations: int = 10\n",
    "    cv_folds: int = 5\n",
    "    significance_level: float = 0.05\n",
    "    \n",
    "    # Effect size thresholds (AUC differences)\n",
    "    minimal_effect: float = 0.005  # 0.5%\n",
    "    small_effect: float = 0.015    # 1.5%\n",
    "    medium_effect: float = 0.025   # 2.5%\n",
    "    large_effect: float = 0.035    # 3.5%\n",
    "    \n",
    "    # Data quality parameters\n",
    "    high_quality_missing: float = 0.02   # 2%\n",
    "    medium_quality_missing: float = 0.10  # 10%\n",
    "    low_quality_missing: float = 0.25     # 25%\n",
    "    \n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate configuration parameters\"\"\"\n",
    "        assert 0 < self.test_size < 1, \"Test size must be between 0 and 1\"\n",
    "        assert self.n_iterations > 0, \"Iterations must be positive\"\n",
    "        assert 0 < self.significance_level < 1, \"Significance level must be between 0 and 1\"\n",
    "        assert (self.minimal_effect < self.small_effect < \n",
    "                self.medium_effect < self.large_effect), \"Effect sizes must be ordered\"\n",
    "\n",
    "def setup_logging() -> logging.Logger:\n",
    "    \"\"\"Single Responsibility: Logging system setup\"\"\"\n",
    "    logger = logging.getLogger('preprocessing_study')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    if not logger.handlers:\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter(\n",
    "            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "            datefmt='%Y-%m-%d %H:%M:%S'\n",
    "        )\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "# Initialize global configuration and logger\n",
    "config = StudyConfiguration()\n",
    "config.validate()\n",
    "logger = setup_logging()\n",
    "\n",
    "np.random.seed(config.random_state)\n",
    "\n",
    "logger.info(f\"Study initialized at {datetime.now()}\")\n",
    "logger.info(f\"Configuration: {config}\")\n",
    "\n",
    "print(\"=== MULTI-DOMAIN PREPROCESSING EFFECTIVENESS STUDY ===\")\n",
    "print(f\"Timestamp: {datetime.now()}\")\n",
    "print(f\"Random seed: {config.random_state}\")\n",
    "print(f\"Effect size thresholds: {config.minimal_effect:.3f} (minimal) to {config.large_effect:.3f} (large)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "077fe9ea-a5f7-4389-abf3-43d30654305e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 12:57:45 - preprocessing_study - INFO - Converted yes_no_column to boolean: {'No': False, 'Yes': True}\n",
      "2025-09-03 12:57:45 - preprocessing_study - INFO - Converted true_false_column to boolean: {'True': True, 'False': False}\n",
      "2025-09-03 12:57:45 - preprocessing_study - INFO - Converted binary_numeric to boolean: {np.int64(1): True, np.int64(0): False}\n",
      "2025-09-03 12:57:45 - preprocessing_study - INFO - Converted male_female to boolean: {'Female': False, 'Male': True}\n",
      "2025-09-03 12:57:45 - preprocessing_study - INFO - Converted high_low to boolean: {'High': True, 'Low': False}\n",
      "2025-09-03 12:57:45 - preprocessing_study - INFO - Converted active_inactive to boolean: {'Active': True, 'Inactive': False}\n",
      "2025-09-03 12:57:45 - preprocessing_study - INFO - Converted categories to category: 3 unique values\n",
      "2025-09-03 12:57:45 - preprocessing_study - INFO - Boolean optimization: 6 columns converted, 76.9% memory reduction\n",
      "2025-09-03 12:57:45 - preprocessing_study - INFO - Total memory optimization: 96.5% reduction (0.33MB → 0.01MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENHANCED DATA TYPE OPTIMIZATION TEST\n",
      "==================================================\n",
      "\n",
      "BEFORE OPTIMIZATION:\n",
      "Memory usage: 0.33 MB\n",
      "Data types:\n",
      "  large_int: int64 (sample: [np.int64(51), np.int64(92), np.int64(14)])\n",
      "  float_data: float64 (sample: [np.float64(0.5868411180208791), np.float64(0.74543947418433), np.float64(0.4316595462296794)])\n",
      "  yes_no_column: object (sample: ['No', 'Yes'])\n",
      "  true_false_column: object (sample: ['True', 'False'])\n",
      "  binary_numeric: int64 (sample: [np.int64(1), np.int64(0)])\n",
      "  male_female: object (sample: ['Female', 'Male'])\n",
      "  categories: object (sample: ['Category_B', 'Category_A', 'Category_C'])\n",
      "  high_low: object (sample: ['High', 'Low'])\n",
      "  active_inactive: object (sample: ['Active', 'Inactive'])\n",
      "\n",
      "AFTER OPTIMIZATION:\n",
      "Memory usage: 0.01 MB\n",
      "Total reduction: 96.5%\n",
      "Boolean optimization alone: 76.9%\n",
      "\n",
      "Optimized data types:\n",
      "  large_int: int8\n",
      "  float_data: float32\n",
      "  yes_no_column: bool\n",
      "  true_false_column: bool\n",
      "  binary_numeric: bool\n",
      "  male_female: bool\n",
      "  categories: category\n",
      "  high_low: bool\n",
      "  active_inactive: bool\n",
      "\n",
      "Boolean conversions made: ['yes_no_column', 'true_false_column', 'binary_numeric', 'male_female', 'high_low', 'active_inactive']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: Enhanced Data Type Optimization Framework with Boolean Detection\n",
    "# =============================================================================\n",
    "\n",
    "class DataTypeOptimizer:\n",
    "    \"\"\"\n",
    "    Single Responsibility: Optimize pandas DataFrame memory usage\n",
    "    KISS: Simple, focused optimization without complex heuristics\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def detect_boolean_candidates(series: pd.Series) -> Tuple[bool, Dict[str, bool]]:\n",
    "        \"\"\"\n",
    "        Detect if a series can be converted to boolean based on common patterns\n",
    "        Returns: (is_boolean_candidate, mapping_dict)\n",
    "        \"\"\"\n",
    "        # Get unique non-null values\n",
    "        unique_values = series.dropna().unique()\n",
    "        \n",
    "        # Skip if more than 2 unique values\n",
    "        if len(unique_values) > 2:\n",
    "            return False, {}\n",
    "        \n",
    "        # Skip if only 1 unique value (constant column)\n",
    "        if len(unique_values) <= 1:\n",
    "            return False, {}\n",
    "        \n",
    "        # Convert to string for pattern matching\n",
    "        str_values = [str(val).lower().strip() for val in unique_values]\n",
    "        str_values_set = set(str_values)\n",
    "        \n",
    "        # Define boolean patterns based on common datasets\n",
    "        boolean_patterns = [\n",
    "            # Yes/No patterns (common in surveys, medical data)\n",
    "            ({'yes', 'no'}, {'yes': True, 'no': False}),\n",
    "            ({'y', 'n'}, {'y': True, 'n': False}),\n",
    "            \n",
    "            # True/False patterns\n",
    "            ({'true', 'false'}, {'true': True, 'false': False}),\n",
    "            ({'t', 'f'}, {'t': True, 'f': False}),\n",
    "            \n",
    "            # Numeric binary patterns\n",
    "            ({'0', '1'}, {'0': False, '1': True}),\n",
    "            ({'0.0', '1.0'}, {'0.0': False, '1.0': True}),\n",
    "            \n",
    "            # Positive/Negative patterns\n",
    "            ({'positive', 'negative'}, {'positive': True, 'negative': False}),\n",
    "            ({'pos', 'neg'}, {'pos': True, 'neg': False}),\n",
    "            \n",
    "            # Present/Absent patterns (common in medical/scientific data)\n",
    "            ({'present', 'absent'}, {'present': True, 'absent': False}),\n",
    "            ({'p', 'a'}, {'p': True, 'a': False}),\n",
    "            \n",
    "            # Success/Failure patterns\n",
    "            ({'success', 'failure'}, {'success': True, 'failure': False}),\n",
    "            ({'pass', 'fail'}, {'pass': True, 'fail': False}),\n",
    "            \n",
    "            # Active/Inactive patterns\n",
    "            ({'active', 'inactive'}, {'active': True, 'inactive': False}),\n",
    "            ({'on', 'off'}, {'on': True, 'off': False}),\n",
    "            \n",
    "            # High/Low patterns\n",
    "            ({'high', 'low'}, {'high': True, 'low': False}),\n",
    "            ({'h', 'l'}, {'h': True, 'l': False}),\n",
    "            \n",
    "            # Male/Female patterns (if appropriate for boolean representation)\n",
    "            ({'male', 'female'}, {'male': True, 'female': False}),\n",
    "            ({'m', 'f'}, {'m': True, 'f': False}),\n",
    "            \n",
    "            # Weekday/Weekend patterns\n",
    "            ({'weekday', 'weekend'}, {'weekday': True, 'weekend': False}),\n",
    "            \n",
    "            # Common abbreviations\n",
    "            ({'good', 'bad'}, {'good': True, 'bad': False}),\n",
    "            ({'up', 'down'}, {'up': True, 'down': False}),\n",
    "        ]\n",
    "        \n",
    "        # Check if values match any boolean pattern\n",
    "        for pattern_set, mapping in boolean_patterns:\n",
    "            if str_values_set == pattern_set:\n",
    "                # Create reverse mapping for original case values\n",
    "                original_mapping = {}\n",
    "                for original_val in unique_values:\n",
    "                    str_val = str(original_val).lower().strip()\n",
    "                    if str_val in mapping:\n",
    "                        original_mapping[original_val] = mapping[str_val]\n",
    "                \n",
    "                return True, original_mapping\n",
    "        \n",
    "        return False, {}\n",
    "    \n",
    "    @staticmethod\n",
    "    def optimize_boolean_columns(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:\n",
    "        \"\"\"Convert appropriate columns to boolean type\"\"\"\n",
    "        df_optimized = df.copy()\n",
    "        converted_columns = []\n",
    "        \n",
    "        for column in df_optimized.columns:\n",
    "            col_data = df_optimized[column]\n",
    "            \n",
    "            # Skip if already boolean\n",
    "            if col_data.dtype == 'bool':\n",
    "                continue\n",
    "            \n",
    "            # Check if column is boolean candidate\n",
    "            is_boolean, mapping = DataTypeOptimizer.detect_boolean_candidates(col_data)\n",
    "            \n",
    "            if is_boolean and mapping:\n",
    "                try:\n",
    "                    # Apply boolean conversion\n",
    "                    df_optimized[column] = col_data.map(mapping).astype('bool')\n",
    "                    converted_columns.append(column)\n",
    "                    logger.info(f\"Converted {column} to boolean: {mapping}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Boolean conversion failed for {column}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        return df_optimized, converted_columns\n",
    "    \n",
    "    @staticmethod\n",
    "    def optimize_numeric_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Optimize numeric columns to smallest possible dtype\"\"\"\n",
    "        df_optimized = df.copy()\n",
    "        \n",
    "        for column in df_optimized.select_dtypes(include=['int64', 'float64']):\n",
    "            col_data = df_optimized[column]\n",
    "            \n",
    "            if col_data.dtype == 'int64':\n",
    "                # Check if can fit in smaller int types\n",
    "                c_min, c_max = col_data.min(), col_data.max()\n",
    "                if c_min >= -128 and c_max <= 127:\n",
    "                    df_optimized[column] = col_data.astype('int8')\n",
    "                elif c_min >= -32768 and c_max <= 32767:\n",
    "                    df_optimized[column] = col_data.astype('int16')\n",
    "                elif c_min >= -2147483648 and c_max <= 32767:\n",
    "                    df_optimized[column] = col_data.astype('int32')\n",
    "            \n",
    "            elif col_data.dtype == 'float64':\n",
    "                # Try to convert to float32 if no precision loss\n",
    "                converted = col_data.astype('float32')\n",
    "                if np.allclose(col_data.values, converted.values, equal_nan=True):\n",
    "                    df_optimized[column] = converted\n",
    "        \n",
    "        return df_optimized\n",
    "    \n",
    "    @staticmethod\n",
    "    def optimize_categorical_columns(df: pd.DataFrame, \n",
    "                                   category_threshold: int = 50) -> pd.DataFrame:\n",
    "        \"\"\"Convert string columns to category if beneficial\"\"\"\n",
    "        df_optimized = df.copy()\n",
    "        \n",
    "        for column in df_optimized.select_dtypes(include=['object']):\n",
    "            unique_values = df_optimized[column].nunique()\n",
    "            total_values = len(df_optimized[column])\n",
    "            \n",
    "            # Convert to category if less than 50% unique values\n",
    "            if unique_values / total_values < 0.5 and unique_values < category_threshold:\n",
    "                df_optimized[column] = df_optimized[column].astype('category')\n",
    "                logger.info(f\"Converted {column} to category: {unique_values} unique values\")\n",
    "        \n",
    "        return df_optimized\n",
    "    \n",
    "    @classmethod\n",
    "    def optimize_dataframe(cls, df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "        \"\"\"Complete DataFrame optimization with comprehensive reporting\"\"\"\n",
    "        initial_memory = df.memory_usage(deep=True).sum() / 1024**2  # MB\n",
    "        \n",
    "        df_optimized = df.copy()\n",
    "        \n",
    "        # Step 1: Boolean optimization (most impactful for binary data)\n",
    "        df_optimized, boolean_conversions = cls.optimize_boolean_columns(df_optimized)\n",
    "        boolean_memory = df_optimized.memory_usage(deep=True).sum() / 1024**2\n",
    "        \n",
    "        # Step 2: Numeric optimization\n",
    "        df_optimized = cls.optimize_numeric_columns(df_optimized)\n",
    "        numeric_memory = df_optimized.memory_usage(deep=True).sum() / 1024**2\n",
    "        \n",
    "        # Step 3: Categorical optimization\n",
    "        df_optimized = cls.optimize_categorical_columns(df_optimized)\n",
    "        final_memory = df_optimized.memory_usage(deep=True).sum() / 1024**2\n",
    "        \n",
    "        total_reduction = (initial_memory - final_memory) / initial_memory * 100\n",
    "        boolean_reduction = (initial_memory - boolean_memory) / initial_memory * 100 if initial_memory > 0 else 0\n",
    "        \n",
    "        optimization_report = {\n",
    "            'initial_memory_mb': initial_memory,\n",
    "            'final_memory_mb': final_memory,\n",
    "            'total_reduction_percent': total_reduction,\n",
    "            'reduction_mb': initial_memory - final_memory,\n",
    "            'boolean_conversions': boolean_conversions,\n",
    "            'boolean_reduction_percent': boolean_reduction,\n",
    "            'optimization_steps': {\n",
    "                'boolean_memory_mb': boolean_memory,\n",
    "                'numeric_memory_mb': numeric_memory,\n",
    "                'categorical_memory_mb': final_memory\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if boolean_conversions:\n",
    "            logger.info(f\"Boolean optimization: {len(boolean_conversions)} columns converted, \"\n",
    "                       f\"{boolean_reduction:.1f}% memory reduction\")\n",
    "        \n",
    "        logger.info(f\"Total memory optimization: {total_reduction:.1f}% reduction \"\n",
    "                   f\"({initial_memory:.2f}MB → {final_memory:.2f}MB)\")\n",
    "        \n",
    "        return df_optimized, optimization_report\n",
    "\n",
    "# Test the enhanced optimizer with boolean cases\n",
    "test_data = pd.DataFrame({\n",
    "    'large_int': np.random.randint(0, 100, 1000),\n",
    "    'float_data': np.random.random(1000),\n",
    "    'yes_no_column': np.random.choice(['Yes', 'No'], 1000),\n",
    "    'true_false_column': np.random.choice(['True', 'False'], 1000),\n",
    "    'binary_numeric': np.random.choice([0, 1], 1000),\n",
    "    'male_female': np.random.choice(['Male', 'Female'], 1000),\n",
    "    'categories': np.random.choice(['Category_A', 'Category_B', 'Category_C'], 1000),\n",
    "    'high_low': np.random.choice(['High', 'Low'], 1000),\n",
    "    'active_inactive': np.random.choice(['Active', 'Inactive'], 1000)\n",
    "})\n",
    "\n",
    "print(\"ENHANCED DATA TYPE OPTIMIZATION TEST\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nBEFORE OPTIMIZATION:\")\n",
    "print(f\"Memory usage: {test_data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(\"Data types:\")\n",
    "for col in test_data.columns:\n",
    "    unique_vals = test_data[col].unique()[:3]  # Show first 3 unique values\n",
    "    print(f\"  {col}: {test_data[col].dtype} (sample: {list(unique_vals)})\")\n",
    "\n",
    "optimized_test, report = DataTypeOptimizer.optimize_dataframe(test_data)\n",
    "\n",
    "print(f\"\\nAFTER OPTIMIZATION:\")\n",
    "print(f\"Memory usage: {report['final_memory_mb']:.2f} MB\")\n",
    "print(f\"Total reduction: {report['total_reduction_percent']:.1f}%\")\n",
    "print(f\"Boolean optimization alone: {report['boolean_reduction_percent']:.1f}%\")\n",
    "\n",
    "print(\"\\nOptimized data types:\")\n",
    "for col in optimized_test.columns:\n",
    "    print(f\"  {col}: {optimized_test[col].dtype}\")\n",
    "\n",
    "if report['boolean_conversions']:\n",
    "    print(f\"\\nBoolean conversions made: {report['boolean_conversions']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4848d64d-d433-44c1-a264-933981afd2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Profiling Framework initialized\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: Dataset Characterization Framework\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class DatasetCharacteristics:\n",
    "    \"\"\"Data class for dataset characteristics\"\"\"\n",
    "    name: str\n",
    "    n_samples: int\n",
    "    n_features: int\n",
    "    target_balance: float\n",
    "    missing_percentage: float\n",
    "    categorical_features: int\n",
    "    numerical_features: int\n",
    "    memory_usage_mb: float\n",
    "    domain: str\n",
    "\n",
    "class DatasetProfiler:\n",
    "    \"\"\"\n",
    "    Single Responsibility: Profile datasets to understand their characteristics\n",
    "    KISS: Simple profiling without complex statistical analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def profile_dataset(X: pd.DataFrame, y: pd.Series, \n",
    "                       name: str, domain: str = \"Unknown\") -> DatasetCharacteristics:\n",
    "        \"\"\"Profile a single dataset\"\"\"\n",
    "        \n",
    "        # Basic statistics\n",
    "        n_samples, n_features = X.shape\n",
    "        target_balance = y.mean() if y.dtype in ['int64', 'bool'] else 0.5\n",
    "        \n",
    "        # Missing data analysis\n",
    "        missing_percentage = (X.isnull().sum().sum() / (n_samples * n_features)) * 100\n",
    "        \n",
    "        # Feature type analysis\n",
    "        categorical_features = len(X.select_dtypes(include=['object', 'category']).columns)\n",
    "        numerical_features = len(X.select_dtypes(include=[np.number]).columns)\n",
    "        \n",
    "        # Memory usage\n",
    "        memory_usage_mb = X.memory_usage(deep=True).sum() / 1024**2\n",
    "        \n",
    "        characteristics = DatasetCharacteristics(\n",
    "            name=name,\n",
    "            n_samples=n_samples,\n",
    "            n_features=n_features,\n",
    "            target_balance=target_balance,\n",
    "            missing_percentage=missing_percentage,\n",
    "            categorical_features=categorical_features,\n",
    "            numerical_features=numerical_features,\n",
    "            memory_usage_mb=memory_usage_mb,\n",
    "            domain=domain\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Dataset {name} profiled: {n_samples:,} samples, \"\n",
    "                   f\"{n_features} features, {missing_percentage:.1f}% missing\")\n",
    "        \n",
    "        return characteristics\n",
    "\n",
    "print(\"Dataset Profiling Framework initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23b7faf6-aca8-4f98-b51b-d0e689f06096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Strategy Framework Initialized\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: Preprocessing Strategy Interface (Strategy Pattern)\n",
    "# =============================================================================\n",
    "\n",
    "class PreprocessingStrategy(Protocol):\n",
    "    \"\"\"\n",
    "    Protocol defining the interface for preprocessing strategies\n",
    "    SOLID: Interface Segregation Principle\n",
    "    \"\"\"\n",
    "    def preprocess(self, X_train: pd.DataFrame, X_test: pd.DataFrame, \n",
    "                  y_train: pd.Series) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Apply preprocessing to training and test sets\"\"\"\n",
    "        ...\n",
    "    \n",
    "    def get_name(self) -> str:\n",
    "        \"\"\"Return strategy name\"\"\"\n",
    "        ...\n",
    "\n",
    "class BasePreprocessingStrategy(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for preprocessing strategies\n",
    "    Single Responsibility: Common preprocessing utilities\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.fitted_transformers = {}\n",
    "    \n",
    "    def get_name(self) -> str:\n",
    "        return self.name\n",
    "    \n",
    "    def _handle_categorical_columns(self, X_train: pd.DataFrame, \n",
    "                                  X_test: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Consistent categorical handling across all strategies\"\"\"\n",
    "        X_train_processed = X_train.copy()\n",
    "        X_test_processed = X_test.copy()\n",
    "        \n",
    "        categorical_cols = X_train_processed.select_dtypes(\n",
    "            include=['object', 'category']).columns\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            # Mode imputation for training set\n",
    "            if X_train_processed[col].isnull().any():\n",
    "                mode_value = X_train_processed[col].mode()\n",
    "                mode_value = mode_value.iloc[0] if len(mode_value) > 0 else 'unknown'\n",
    "                X_train_processed[col] = X_train_processed[col].fillna(mode_value)\n",
    "                X_test_processed[col] = X_test_processed[col].fillna(mode_value)\n",
    "            \n",
    "            # Convert to string and create label encoding\n",
    "            X_train_processed[col] = X_train_processed[col].astype(str)\n",
    "            X_test_processed[col] = X_test_processed[col].astype(str)\n",
    "            \n",
    "            # Fit encoder on training set\n",
    "            unique_values = X_train_processed[col].unique()\n",
    "            mapping = {val: float(idx) for idx, val in enumerate(unique_values)}\n",
    "            \n",
    "            # Handle unseen categories in test set\n",
    "            test_unique = set(X_test_processed[col].unique())\n",
    "            train_unique = set(unique_values)\n",
    "            unseen_categories = test_unique - train_unique\n",
    "            \n",
    "            if unseen_categories:\n",
    "                logger.warning(f\"Column {col}: {len(unseen_categories)} unseen categories in test set\")\n",
    "                # Map unseen categories to a default value\n",
    "                for unseen_cat in unseen_categories:\n",
    "                    mapping[unseen_cat] = float(len(unique_values))  # New index\n",
    "            \n",
    "            # Apply encoding\n",
    "            X_train_processed[col] = X_train_processed[col].map(mapping).astype(float)\n",
    "            X_test_processed[col] = X_test_processed[col].map(mapping).astype(float)\n",
    "        \n",
    "        return X_train_processed, X_test_processed\n",
    "    \n",
    "    @abstractmethod\n",
    "    def preprocess(self, X_train: pd.DataFrame, X_test: pd.DataFrame, \n",
    "                  y_train: pd.Series) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        pass\n",
    "\n",
    "print(\"Preprocessing Strategy Framework Initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7c0c5c5-0da9-464f-87a9-bfa3ba0ef989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Strategies Implemented:\n",
      "- Minimal\n",
      "- Standard\n",
      "- Advanced\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: Concrete Preprocessing Strategies\n",
    "# =============================================================================\n",
    "\n",
    "class MinimalPreprocessingStrategy(BasePreprocessingStrategy):\n",
    "    \"\"\"\n",
    "    Single Responsibility: Basic preprocessing with minimal intervention\n",
    "    KISS: Simplest approach - only handle missing values\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"Minimal\")\n",
    "    \n",
    "    def preprocess(self, X_train: pd.DataFrame, X_test: pd.DataFrame, \n",
    "                  y_train: pd.Series) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Apply minimal preprocessing\"\"\"\n",
    "        try:\n",
    "            X_train_processed = X_train.copy()\n",
    "            X_test_processed = X_test.copy()\n",
    "            \n",
    "            # Handle numeric columns - simple mean imputation\n",
    "            numeric_cols = X_train_processed.select_dtypes(include=[np.number]).columns\n",
    "            if len(numeric_cols) > 0:\n",
    "                imputer = SimpleImputer(strategy='mean')\n",
    "                X_train_processed[numeric_cols] = imputer.fit_transform(X_train_processed[numeric_cols])\n",
    "                X_test_processed[numeric_cols] = imputer.transform(X_test_processed[numeric_cols])\n",
    "            \n",
    "            # Handle categorical columns\n",
    "            X_train_processed, X_test_processed = self._handle_categorical_columns(\n",
    "                X_train_processed, X_test_processed)\n",
    "            \n",
    "            logger.debug(f\"Minimal preprocessing completed: {X_train_processed.shape}\")\n",
    "            return X_train_processed, X_test_processed\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Minimal preprocessing failed: {str(e)}\")\n",
    "            return X_train.copy(), X_test.copy()\n",
    "\n",
    "class StandardPreprocessingStrategy(BasePreprocessingStrategy):\n",
    "    \"\"\"\n",
    "    Single Responsibility: Standard industry preprocessing practices\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"Standard\")\n",
    "    \n",
    "    def preprocess(self, X_train: pd.DataFrame, X_test: pd.DataFrame, \n",
    "                  y_train: pd.Series) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Apply standard preprocessing\"\"\"\n",
    "        try:\n",
    "            X_train_processed = X_train.copy()\n",
    "            X_test_processed = X_test.copy()\n",
    "            \n",
    "            # Handle numeric columns - imputation + standardization\n",
    "            numeric_cols = X_train_processed.select_dtypes(include=[np.number]).columns\n",
    "            if len(numeric_cols) > 0:\n",
    "                # Imputation\n",
    "                imputer = SimpleImputer(strategy='mean')\n",
    "                X_train_processed[numeric_cols] = imputer.fit_transform(X_train_processed[numeric_cols])\n",
    "                X_test_processed[numeric_cols] = imputer.transform(X_test_processed[numeric_cols])\n",
    "                \n",
    "                # Scaling\n",
    "                scaler = StandardScaler()\n",
    "                X_train_processed[numeric_cols] = scaler.fit_transform(X_train_processed[numeric_cols])\n",
    "                X_test_processed[numeric_cols] = scaler.transform(X_test_processed[numeric_cols])\n",
    "            \n",
    "            # Handle categorical columns\n",
    "            X_train_processed, X_test_processed = self._handle_categorical_columns(\n",
    "                X_train_processed, X_test_processed)\n",
    "            \n",
    "            logger.debug(f\"Standard preprocessing completed: {X_train_processed.shape}\")\n",
    "            return X_train_processed, X_test_processed\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Standard preprocessing failed: {str(e)}\")\n",
    "            return X_train.copy(), X_test.copy()\n",
    "\n",
    "class AdvancedPreprocessingStrategy(BasePreprocessingStrategy):\n",
    "    \"\"\"\n",
    "    Single Responsibility: Advanced preprocessing with KNN imputation and robust scaling\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"Advanced\")\n",
    "    \n",
    "    def preprocess(self, X_train: pd.DataFrame, X_test: pd.DataFrame, \n",
    "                  y_train: pd.Series) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Apply advanced preprocessing\"\"\"\n",
    "        try:\n",
    "            X_train_processed = X_train.copy()\n",
    "            X_test_processed = X_test.copy()\n",
    "            \n",
    "            # Handle numeric columns - KNN imputation + robust scaling\n",
    "            numeric_cols = X_train_processed.select_dtypes(include=[np.number]).columns\n",
    "            if len(numeric_cols) > 0:\n",
    "                # Conservative K selection for KNN imputation\n",
    "                n_samples = len(X_train_processed)\n",
    "                k_neighbors = min(5, max(1, n_samples // 1000))\n",
    "                \n",
    "                # KNN Imputation\n",
    "                imputer = KNNImputer(n_neighbors=k_neighbors)\n",
    "                X_train_processed[numeric_cols] = imputer.fit_transform(X_train_processed[numeric_cols])\n",
    "                X_test_processed[numeric_cols] = imputer.transform(X_test_processed[numeric_cols])\n",
    "                \n",
    "                # Robust scaling (less sensitive to outliers)\n",
    "                scaler = RobustScaler()\n",
    "                X_train_processed[numeric_cols] = scaler.fit_transform(X_train_processed[numeric_cols])\n",
    "                X_test_processed[numeric_cols] = scaler.transform(X_test_processed[numeric_cols])\n",
    "            \n",
    "            # Handle categorical columns\n",
    "            X_train_processed, X_test_processed = self._handle_categorical_columns(\n",
    "                X_train_processed, X_test_processed)\n",
    "            \n",
    "            logger.debug(f\"Advanced preprocessing completed: {X_train_processed.shape}\")\n",
    "            return X_train_processed, X_test_processed\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Advanced preprocessing failed: {str(e)}\")\n",
    "            return X_train.copy(), X_test.copy()\n",
    "\n",
    "# Initialize preprocessing strategies\n",
    "strategies = [\n",
    "    MinimalPreprocessingStrategy(),\n",
    "    StandardPreprocessingStrategy(), \n",
    "    AdvancedPreprocessingStrategy()\n",
    "]\n",
    "\n",
    "print(\"Preprocessing Strategies Implemented:\")\n",
    "for strategy in strategies:\n",
    "    print(f\"- {strategy.get_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "028f7aa3-bf1e-43a1-8171-16c419d1b231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality Degradation System initialized\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 6: Data Quality Degradation System\n",
    "# =============================================================================\n",
    "\n",
    "class DataQualityDegrader:\n",
    "    \"\"\"\n",
    "    Single Responsibility: Systematically degrade data quality for experimental control\n",
    "    KISS: Simple, transparent quality reduction\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def degrade_data_quality(X: pd.DataFrame, y: pd.Series, \n",
    "                           missing_rate: float,\n",
    "                           random_state: int = 42) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        Degrade data quality by introducing missing values\n",
    "        \n",
    "        Args:\n",
    "            X: Features DataFrame\n",
    "            y: Target Series (unchanged)\n",
    "            missing_rate: Proportion of values to make missing\n",
    "            random_state: Random seed for reproducibility\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of degraded features and unchanged target\n",
    "        \"\"\"\n",
    "        if missing_rate <= 0:\n",
    "            return X.copy(), y.copy()\n",
    "        \n",
    "        np.random.seed(random_state)\n",
    "        X_degraded = X.copy()\n",
    "        \n",
    "        try:\n",
    "            # Only introduce missing values in numeric columns for controlled degradation\n",
    "            numeric_cols = X_degraded.select_dtypes(include=[np.number]).columns\n",
    "            \n",
    "            if len(numeric_cols) > 0:\n",
    "                n_total_values = len(X_degraded) * len(numeric_cols)\n",
    "                n_missing = int(n_total_values * missing_rate)\n",
    "                \n",
    "                # Distribute missing values across numeric columns\n",
    "                for col in numeric_cols:\n",
    "                    col_missing = n_missing // len(numeric_cols)\n",
    "                    if col_missing > 0:\n",
    "                        missing_indices = np.random.choice(\n",
    "                            len(X_degraded), \n",
    "                            min(col_missing, len(X_degraded)), \n",
    "                            replace=False\n",
    "                        )\n",
    "                        X_degraded.loc[X_degraded.index[missing_indices], col] = np.nan\n",
    "                \n",
    "                logger.debug(f\"Data quality degraded: {missing_rate:.1%} missing values introduced\")\n",
    "            \n",
    "            return X_degraded, y.copy()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Data quality degradation failed: {str(e)}\")\n",
    "            return X.copy(), y.copy()\n",
    "\n",
    "print(\"Data Quality Degradation System initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfda4dec-dbe5-4373-ad0d-296009f4f0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validated Experiment Executor initialized\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 7: Experiment Results and Execution Framework\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class ExperimentResult:\n",
    "    \"\"\"Data class for experiment results\"\"\"\n",
    "    dataset_name: str\n",
    "    strategy_name: str\n",
    "    quality_level: str\n",
    "    auc_score: float\n",
    "    execution_time: float\n",
    "    memory_usage_mb: float\n",
    "    success: bool\n",
    "    error_message: Optional[str] = None\n",
    "\n",
    "@dataclass\n",
    "class ComputationalCostMetrics:\n",
    "    \"\"\"Data class for computational cost tracking\"\"\"\n",
    "    preprocessing_time_seconds: float\n",
    "    training_time_seconds: float\n",
    "    prediction_time_seconds: float\n",
    "    memory_overhead_mb: float\n",
    "    time_per_sample_ms: float\n",
    "    memory_per_sample_kb: float\n",
    "\n",
    "class CrossValidatedExperimentExecutor:\n",
    "    \"\"\"\n",
    "    Single Responsibility: Execute cross-validated experiments with cost tracking\n",
    "    KISS: Simple k-fold CV with comprehensive metrics collection\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: StudyConfiguration, cv_folds: int = 5):\n",
    "        self.config = config\n",
    "        self.cv_folds = cv_folds\n",
    "    \n",
    "    def calculate_computational_cost(self, X_before: pd.DataFrame, X_after: pd.DataFrame,\n",
    "                                   preprocessing_time: float, training_time: float,\n",
    "                                   prediction_time: float) -> ComputationalCostMetrics:\n",
    "        \"\"\"Calculate comprehensive computational cost metrics\"\"\"\n",
    "        \n",
    "        n_samples = len(X_before)\n",
    "        memory_before = X_before.memory_usage(deep=True).sum() / 1024**2  # MB\n",
    "        memory_after = X_after.memory_usage(deep=True).sum() / 1024**2   # MB\n",
    "        memory_overhead = memory_after - memory_before\n",
    "        \n",
    "        return ComputationalCostMetrics(\n",
    "            preprocessing_time_seconds=preprocessing_time,\n",
    "            training_time_seconds=training_time,\n",
    "            prediction_time_seconds=prediction_time,\n",
    "            memory_overhead_mb=memory_overhead,\n",
    "            time_per_sample_ms=(preprocessing_time * 1000) / n_samples,\n",
    "            memory_per_sample_kb=(memory_overhead * 1024) / n_samples if n_samples > 0 else 0.0\n",
    "        )\n",
    "    \n",
    "    def execute_cv_experiment(self, \n",
    "                            X: pd.DataFrame, \n",
    "                            y: pd.Series,\n",
    "                            dataset_name: str,\n",
    "                            strategy: PreprocessingStrategy,\n",
    "                            quality_level: str,\n",
    "                            missing_rate: float,\n",
    "                            random_state: int) -> Dict[str, Any]:\n",
    "        \"\"\"Execute cross-validated experiment with cost tracking\"\"\"\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        try:\n",
    "            # Degrade data quality\n",
    "            X_degraded, y_unchanged = DataQualityDegrader.degrade_data_quality(\n",
    "                X, y, missing_rate, random_state\n",
    "            )\n",
    "            \n",
    "            # Initialize cross-validation\n",
    "            cv = StratifiedKFold(n_splits=self.cv_folds, shuffle=True, random_state=random_state)\n",
    "            \n",
    "            cv_scores = []\n",
    "            cost_metrics = []\n",
    "            \n",
    "            fold = 0\n",
    "            for train_idx, test_idx in cv.split(X_degraded, y_unchanged):\n",
    "                fold += 1\n",
    "                \n",
    "                # Split data\n",
    "                X_train = X_degraded.iloc[train_idx].reset_index(drop=True)\n",
    "                X_test = X_degraded.iloc[test_idx].reset_index(drop=True)\n",
    "                y_train = y_unchanged.iloc[train_idx].reset_index(drop=True)\n",
    "                y_test = y_unchanged.iloc[test_idx].reset_index(drop=True)\n",
    "                \n",
    "                # Apply preprocessing with timing\n",
    "                prep_start = datetime.now()\n",
    "                X_train_processed, X_test_processed = strategy.preprocess(X_train, X_test, y_train)\n",
    "                preprocessing_time = (datetime.now() - prep_start).total_seconds()\n",
    "                \n",
    "                # Train model with timing\n",
    "                train_start = datetime.now()\n",
    "                model = LogisticRegression(\n",
    "                    random_state=random_state,\n",
    "                    max_iter=1000,\n",
    "                    solver='liblinear'\n",
    "                )\n",
    "                model.fit(X_train_processed, y_train)\n",
    "                training_time = (datetime.now() - train_start).total_seconds()\n",
    "                \n",
    "                # Predict with timing\n",
    "                pred_start = datetime.now()\n",
    "                y_pred_proba = model.predict_proba(X_test_processed)[:, 1]\n",
    "                prediction_time = (datetime.now() - pred_start).total_seconds()\n",
    "                \n",
    "                # Calculate metrics\n",
    "                auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "                cv_scores.append(auc_score)\n",
    "                \n",
    "                # Calculate computational costs\n",
    "                fold_cost = self.calculate_computational_cost(\n",
    "                    X_train, X_train_processed, preprocessing_time, \n",
    "                    training_time, prediction_time\n",
    "                )\n",
    "                cost_metrics.append(fold_cost)\n",
    "                \n",
    "                # Memory cleanup\n",
    "                del X_train_processed, X_test_processed, model\n",
    "                gc.collect()\n",
    "            \n",
    "            # Aggregate results\n",
    "            total_time = (datetime.now() - start_time).total_seconds()\n",
    "            \n",
    "            # Calculate mean and std of CV scores\n",
    "            mean_auc = np.mean(cv_scores)\n",
    "            std_auc = np.std(cv_scores, ddof=1)\n",
    "            \n",
    "            # Aggregate computational costs\n",
    "            mean_cost = ComputationalCostMetrics(\n",
    "                preprocessing_time_seconds=np.mean([c.preprocessing_time_seconds for c in cost_metrics]),\n",
    "                training_time_seconds=np.mean([c.training_time_seconds for c in cost_metrics]),\n",
    "                prediction_time_seconds=np.mean([c.prediction_time_seconds for c in cost_metrics]),\n",
    "                memory_overhead_mb=np.mean([c.memory_overhead_mb for c in cost_metrics]),\n",
    "                time_per_sample_ms=np.mean([c.time_per_sample_ms for c in cost_metrics]),\n",
    "                memory_per_sample_kb=np.mean([c.memory_per_sample_kb for c in cost_metrics])\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'dataset_name': dataset_name,\n",
    "                'strategy_name': strategy.get_name(),\n",
    "                'quality_level': quality_level,\n",
    "                'cv_scores': cv_scores,\n",
    "                'mean_auc': mean_auc,\n",
    "                'std_auc': std_auc,\n",
    "                'min_auc': np.min(cv_scores),\n",
    "                'max_auc': np.max(cv_scores),\n",
    "                'total_execution_time': total_time,\n",
    "                'computational_cost': mean_cost,\n",
    "                'n_folds': self.cv_folds,\n",
    "                'success': True\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            total_time = (datetime.now() - start_time).total_seconds()\n",
    "            return {\n",
    "                'dataset_name': dataset_name,\n",
    "                'strategy_name': strategy.get_name(),\n",
    "                'quality_level': quality_level,\n",
    "                'cv_scores': [0.5] * self.cv_folds,\n",
    "                'mean_auc': 0.5,\n",
    "                'std_auc': 0.0,\n",
    "                'total_execution_time': total_time,\n",
    "                'computational_cost': ComputationalCostMetrics(0, 0, 0, 0, 0, 0),\n",
    "                'success': False,\n",
    "                'error_message': str(e)\n",
    "            }\n",
    "\n",
    "print(\"Cross-Validated Experiment Executor initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "045c0329-a86d-4e68-860b-3d13031090fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 12:57:45 - preprocessing_study - INFO - Loading 11 UCI datasets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading UCI datasets...\n",
      "================================================================================\n",
      "LOADING AND OPTIMIZING DATASETS - EDUCATIONAL OVERVIEW\n",
      "================================================================================\n",
      "This section demonstrates the complete data loading and optimization process\n",
      "for reproducibility and practitioner guidance.\n",
      "\n",
      "\n",
      "============================================================\n",
      "DATASET 1/11: ADULT INCOME\n",
      "============================================================\n",
      "Domain: Socioeconomic\n",
      "UCI ID: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 12:57:47 - preprocessing_study - ERROR - Failed to load Adult Income: DataTypeOptimizer.optimize_dataframe() got an unexpected keyword argument 'preserve_object_types'\n",
      "2025-09-03 12:57:47 - preprocessing_study - ERROR - ✗ Adult Income failed to load: DataTypeOptimizer.optimize_dataframe() got an unexpected keyword argument 'preserve_object_types'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌ Loading failed: DataTypeOptimizer.optimize_dataframe() got an unexpected keyword argument 'preserve_object_types'\n",
      "\n",
      "============================================================\n",
      "DATASET 2/11: BANK MARKETING\n",
      "============================================================\n",
      "Domain: Financial Services\n",
      "UCI ID: 222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 12:57:49 - preprocessing_study - ERROR - Failed to load Bank Marketing: DataTypeOptimizer.optimize_dataframe() got an unexpected keyword argument 'preserve_object_types'\n",
      "2025-09-03 12:57:49 - preprocessing_study - ERROR - ✗ Bank Marketing failed to load: DataTypeOptimizer.optimize_dataframe() got an unexpected keyword argument 'preserve_object_types'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌ Loading failed: DataTypeOptimizer.optimize_dataframe() got an unexpected keyword argument 'preserve_object_types'\n",
      "\n",
      "============================================================\n",
      "DATASET 3/11: FOREST COVER TYPE\n",
      "============================================================\n",
      "Domain: Environmental\n",
      "UCI ID: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 12:57:58 - preprocessing_study - INFO - Sampled Forest Cover Type from 581,012 to 50,000 samples\n",
      "2025-09-03 12:57:58 - preprocessing_study - ERROR - Failed to load Forest Cover Type: DataTypeOptimizer.optimize_dataframe() got an unexpected keyword argument 'preserve_object_types'\n",
      "2025-09-03 12:57:58 - preprocessing_study - ERROR - ✗ Forest Cover Type failed to load: DataTypeOptimizer.optimize_dataframe() got an unexpected keyword argument 'preserve_object_types'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌ Loading failed: DataTypeOptimizer.optimize_dataframe() got an unexpected keyword argument 'preserve_object_types'\n",
      "\n",
      "============================================================\n",
      "DATASET 4/11: ELECTRIC POWER\n",
      "============================================================\n",
      "Domain: Utilities\n",
      "UCI ID: 235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 12:58:10 - preprocessing_study - ERROR - Failed to load Electric Power: 'NoneType' object has no attribute 'copy'\n",
      "2025-09-03 12:58:10 - preprocessing_study - ERROR - ✗ Electric Power failed to load: 'NoneType' object has no attribute 'copy'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌ Loading failed: 'NoneType' object has no attribute 'copy'\n",
      "\n",
      "============================================================\n",
      "DATASET 5/11: DIABETES HOSPITALS\n",
      "============================================================\n",
      "Domain: Healthcare\n",
      "UCI ID: 296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 12:58:13 - preprocessing_study - INFO - Sampled Diabetes Hospitals from 101,766 to 50,000 samples\n",
      "2025-09-03 12:58:13 - preprocessing_study - ERROR - Failed to load Diabetes Hospitals: DataTypeOptimizer.optimize_dataframe() got an unexpected keyword argument 'preserve_object_types'\n",
      "2025-09-03 12:58:13 - preprocessing_study - ERROR - ✗ Diabetes Hospitals failed to load: DataTypeOptimizer.optimize_dataframe() got an unexpected keyword argument 'preserve_object_types'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌ Loading failed: DataTypeOptimizer.optimize_dataframe() got an unexpected keyword argument 'preserve_object_types'\n",
      "\n",
      "============================================================\n",
      "DATASET 6/11: POKER HAND\n",
      "============================================================\n",
      "Domain: Gaming Analytics\n",
      "UCI ID: 158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 12:58:15 - preprocessing_study - ERROR - Failed to load Poker Hand: DataTypeOptimizer.optimize_dataframe() got an unexpected keyword argument 'preserve_object_types'\n",
      "2025-09-03 12:58:15 - preprocessing_study - ERROR - ✗ Poker Hand failed to load: DataTypeOptimizer.optimize_dataframe() got an unexpected keyword argument 'preserve_object_types'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌ Loading failed: DataTypeOptimizer.optimize_dataframe() got an unexpected keyword argument 'preserve_object_types'\n",
      "\n",
      "============================================================\n",
      "DATASET 7/11: BIKE SHARING DC\n",
      "============================================================\n",
      "Domain: Transportation\n",
      "UCI ID: 275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 12:58:16 - preprocessing_study - ERROR - Failed to load Bike Sharing DC: DataTypeOptimizer.optimize_dataframe() got an unexpected keyword argument 'preserve_object_types'\n",
      "2025-09-03 12:58:16 - preprocessing_study - ERROR - ✗ Bike Sharing DC failed to load: DataTypeOptimizer.optimize_dataframe() got an unexpected keyword argument 'preserve_object_types'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌ Loading failed: DataTypeOptimizer.optimize_dataframe() got an unexpected keyword argument 'preserve_object_types'\n",
      "\n",
      "============================================================\n",
      "DATASET 8/11: SEOUL BIKE SHARING\n",
      "============================================================\n",
      "Domain: Urban Planning\n",
      "UCI ID: 560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 12:58:17 - preprocessing_study - ERROR - Failed to load Seoul Bike Sharing: DataTypeOptimizer.optimize_dataframe() got an unexpected keyword argument 'preserve_object_types'\n",
      "2025-09-03 12:58:17 - preprocessing_study - ERROR - ✗ Seoul Bike Sharing failed to load: DataTypeOptimizer.optimize_dataframe() got an unexpected keyword argument 'preserve_object_types'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌ Loading failed: DataTypeOptimizer.optimize_dataframe() got an unexpected keyword argument 'preserve_object_types'\n",
      "\n",
      "============================================================\n",
      "DATASET 9/11: MUSHROOM\n",
      "============================================================\n",
      "Domain: Food Safety\n",
      "UCI ID: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 12:58:18 - preprocessing_study - ERROR - Failed to load Mushroom: DataTypeOptimizer.optimize_dataframe() got an unexpected keyword argument 'preserve_object_types'\n",
      "2025-09-03 12:58:18 - preprocessing_study - ERROR - ✗ Mushroom failed to load: DataTypeOptimizer.optimize_dataframe() got an unexpected keyword argument 'preserve_object_types'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌ Loading failed: DataTypeOptimizer.optimize_dataframe() got an unexpected keyword argument 'preserve_object_types'\n",
      "\n",
      "============================================================\n",
      "DATASET 10/11: WINE QUALITY\n",
      "============================================================\n",
      "Domain: Manufacturing\n",
      "UCI ID: 186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 12:58:19 - preprocessing_study - ERROR - Failed to load Wine Quality: DataTypeOptimizer.optimize_dataframe() got an unexpected keyword argument 'preserve_object_types'\n",
      "2025-09-03 12:58:19 - preprocessing_study - ERROR - ✗ Wine Quality failed to load: DataTypeOptimizer.optimize_dataframe() got an unexpected keyword argument 'preserve_object_types'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌ Loading failed: DataTypeOptimizer.optimize_dataframe() got an unexpected keyword argument 'preserve_object_types'\n",
      "\n",
      "============================================================\n",
      "DATASET 11/11: SPAMBASE\n",
      "============================================================\n",
      "Domain: Cybersecurity\n",
      "UCI ID: 94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 12:58:20 - preprocessing_study - ERROR - Failed to load Spambase: DataTypeOptimizer.optimize_dataframe() got an unexpected keyword argument 'preserve_object_types'\n",
      "2025-09-03 12:58:20 - preprocessing_study - ERROR - ✗ Spambase failed to load: DataTypeOptimizer.optimize_dataframe() got an unexpected keyword argument 'preserve_object_types'\n",
      "2025-09-03 12:58:20 - preprocessing_study - INFO - Successfully loaded 0/11 datasets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌ Loading failed: DataTypeOptimizer.optimize_dataframe() got an unexpected keyword argument 'preserve_object_types'\n",
      "\n",
      "================================================================================\n",
      "DATASET LOADING SUMMARY\n",
      "================================================================================\n",
      "Successfully loaded: 0/11 datasets\n",
      "⚠️  No datasets successfully loaded\n",
      "================================================================================\n",
      "\n",
      "Dataset loading completed: 0 datasets available\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 8: UCI Dataset Loading System (Factory Pattern)\n",
    "# =============================================================================\n",
    "\n",
    "class DatasetLoader(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base for dataset loaders\n",
    "    Single Responsibility: Define dataset loading interface\n",
    "    \"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def load(self) -> Tuple[pd.DataFrame, pd.Series, str]:\n",
    "        \"\"\"Load dataset and return features, target, description\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_domain(self) -> str:\n",
    "        \"\"\"Return dataset domain category\"\"\"\n",
    "        pass\n",
    "\n",
    "class UCIDatasetLoader(DatasetLoader):\n",
    "    \"\"\"\n",
    "    Single Responsibility: Load UCI datasets with consistent error handling\n",
    "    KISS: Simple, reliable loading matching original study's successful patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_id: int, name: str, domain: str, \n",
    "                 sample_size_limit: int = None):\n",
    "        self.dataset_id = dataset_id\n",
    "        self.name = name\n",
    "        self.domain = domain\n",
    "        self.sample_size_limit = sample_size_limit\n",
    "    \n",
    "    def _handle_target_conversion(self, y: pd.Series, dataset_name: str) -> pd.Series:\n",
    "        \"\"\"Handle target variable conversion based on original study patterns\"\"\"\n",
    "        \n",
    "        if dataset_name == \"Adult Income\":\n",
    "            return (y == '>50K').astype(int)\n",
    "        elif dataset_name == \"Bank Marketing\":\n",
    "            return (y == 'yes').astype(int)\n",
    "        elif dataset_name == \"Forest Cover Type\":\n",
    "            # Convert to binary: cover type 1 vs others (as in original)\n",
    "            return (y == 1).astype(int)\n",
    "        elif dataset_name == \"Electric Power\":\n",
    "            # High vs low consumption based on 75th percentile\n",
    "            if y.dtype == 'object':\n",
    "                y_numeric = pd.to_numeric(y, errors='coerce')\n",
    "                y_clean = y_numeric.dropna()\n",
    "                if len(y_clean) >= 1000:\n",
    "                    threshold = y_clean.quantile(0.75)\n",
    "                    return (y_numeric >= threshold).fillna(0).astype(int)\n",
    "            else:\n",
    "                threshold = y.quantile(0.75)\n",
    "                return (y >= threshold).astype(int)\n",
    "        elif dataset_name == \"Diabetes Hospitals\":\n",
    "            # Readmission vs no readmission\n",
    "            return (y != 'NO').astype(int)\n",
    "        elif dataset_name == \"Poker Hand\":\n",
    "            # Pair or better vs nothing\n",
    "            return (y > 0).astype(int)\n",
    "        elif dataset_name == \"Bike Sharing DC\":\n",
    "            # High vs low demand (top 25%)\n",
    "            threshold = y.quantile(0.75)\n",
    "            return (y >= threshold).astype(int)\n",
    "        elif dataset_name == \"Seoul Bike Sharing\":\n",
    "            # Handle Yes/No target\n",
    "            if y.dtype == 'object' and 'Yes' in str(y.unique()):\n",
    "                return (y == 'Yes').astype(int)\n",
    "            else:\n",
    "                return (y >= y.median()).astype(int)\n",
    "        elif dataset_name == \"Mushroom\":\n",
    "            # Edible vs poisonous\n",
    "            return (y == 'e').astype(int)\n",
    "        elif dataset_name == \"Wine Quality\":\n",
    "            # High quality (7+) vs standard\n",
    "            return (y >= 7).astype(int)\n",
    "        elif dataset_name == \"Spambase\":\n",
    "            # Already binary\n",
    "            return y.astype(int)\n",
    "        else:\n",
    "            # Default binary conversion\n",
    "            if y.dtype == 'object':\n",
    "                unique_values = y.unique()\n",
    "                if len(unique_values) == 2:\n",
    "                    return (y == unique_values[1]).astype(int)\n",
    "            return (y >= y.median()).astype(int)\n",
    "    \n",
    "    def _apply_sampling_if_needed(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"Apply sampling for very large datasets as done in original study\"\"\"\n",
    "        \n",
    "        if self.sample_size_limit and len(X) > self.sample_size_limit:\n",
    "            np.random.seed(42)  # Fixed seed for reproducibility\n",
    "            sample_idx = np.random.choice(len(X), self.sample_size_limit, replace=False)\n",
    "            X_sampled = X.iloc[sample_idx].reset_index(drop=True)\n",
    "            y_sampled = y.iloc[sample_idx].reset_index(drop=True)\n",
    "            logger.info(f\"Sampled {self.name} from {len(X):,} to {len(X_sampled):,} samples\")\n",
    "            return X_sampled, y_sampled\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def load(self) -> Tuple[pd.DataFrame, pd.Series, str]:\n",
    "        \"\"\"Load UCI dataset with robust error handling matching original study\"\"\"\n",
    "        try:\n",
    "            if not UCI_AVAILABLE:\n",
    "                raise ImportError(\"ucimlrepo package not available\")\n",
    "            \n",
    "            # Fetch dataset\n",
    "            dataset = fetch_ucirepo(id=self.dataset_id)\n",
    "            X = dataset.data.features.copy()\n",
    "            y = dataset.data.targets.copy()\n",
    "            \n",
    "            # Handle target variable\n",
    "            if y.shape[1] > 1:\n",
    "                y = y.iloc[:, 0]\n",
    "            else:\n",
    "                y = y.squeeze()\n",
    "            \n",
    "            # Apply dataset-specific sampling if needed\n",
    "            if self.sample_size_limit:\n",
    "                X, y = self._apply_sampling_if_needed(X, y)\n",
    "            \n",
    "            # Convert target to binary\n",
    "            y_binary = self._handle_target_conversion(y, self.name)\n",
    "            \n",
    "            # Clean feature data (remove target-leaking features)\n",
    "            if self.name == \"Bike Sharing DC\":\n",
    "                # Remove casual, registered, instant, dteday as in original\n",
    "                features_to_remove = ['casual', 'registered', 'instant', 'dteday']\n",
    "                X = X.drop(columns=[col for col in features_to_remove if col in X.columns])\n",
    "            elif self.name == \"Electric Power\":\n",
    "                # Remove temporal identifiers\n",
    "                X = X.drop(columns=['Date', 'Time'], errors='ignore')\n",
    "                # Remove target column if it exists in features\n",
    "                X = X.drop(columns=['Global_active_power'], errors='ignore')\n",
    "            \n",
    "            # Optimize data types before profiling - preserve object types for stability\n",
    "            X_optimized, optimization_report = DataTypeOptimizer.optimize_dataframe(\n",
    "                X, preserve_object_types=True\n",
    "            )\n",
    "            \n",
    "            description = (f\"{self.name}: {len(X_optimized):,} samples, \"\n",
    "                         f\"{len(X_optimized.columns)} features, \"\n",
    "                         f\"{y_binary.mean():.1%} positive class ({self.domain})\")\n",
    "            \n",
    "            logger.info(f\"Loaded {self.name}: {len(X_optimized):,} samples, \"\n",
    "                       f\"memory optimized by {optimization_report['total_reduction_percent']:.1f}%\")\n",
    "            \n",
    "            return X_optimized, y_binary, description\n",
    "            \n",
    "        except ImportError:\n",
    "            logger.error(f\"ucimlrepo package required for {self.name}\")\n",
    "            raise ImportError(\"Install ucimlrepo: pip install ucimlrepo\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load {self.name}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def get_domain(self) -> str:\n",
    "        return self.domain\n",
    "\n",
    "class DatasetFactory:\n",
    "    \"\"\"\n",
    "    Factory Pattern: Create dataset loaders based on original study\n",
    "    Single Responsibility: Centralized dataset configuration matching proven datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_uci_loaders() -> List[DatasetLoader]:\n",
    "        \"\"\"Create UCI dataset loaders based on successfully loaded datasets from original study\"\"\"\n",
    "        \n",
    "        loaders = [\n",
    "            # Successfully loaded large datasets from original study\n",
    "            UCIDatasetLoader(2, \"Adult Income\", \"Socioeconomic\"),                    # 48,842 samples\n",
    "            UCIDatasetLoader(222, \"Bank Marketing\", \"Financial Services\"),          # 45,211 samples  \n",
    "            UCIDatasetLoader(31, \"Forest Cover Type\", \"Environmental\", 50000),      # Sample to 50K\n",
    "            UCIDatasetLoader(235, \"Electric Power\", \"Utilities\", 50000),            # Sample to 50K\n",
    "            UCIDatasetLoader(296, \"Diabetes Hospitals\", \"Healthcare\", 50000),       # Sample to 50K\n",
    "            UCIDatasetLoader(158, \"Poker Hand\", \"Gaming Analytics\"),                # Keep full size (1M+)\n",
    "            UCIDatasetLoader(275, \"Bike Sharing DC\", \"Transportation\"),             # 17,379 samples\n",
    "            UCIDatasetLoader(560, \"Seoul Bike Sharing\", \"Urban Planning\"),          # 8,760 samples\n",
    "            UCIDatasetLoader(73, \"Mushroom\", \"Food Safety\"),                        # 8,124 samples\n",
    "            UCIDatasetLoader(186, \"Wine Quality\", \"Manufacturing\"),                 # 6,497 samples\n",
    "            UCIDatasetLoader(94, \"Spambase\", \"Cybersecurity\"),                      # 4,601 samples\n",
    "        ]\n",
    "        \n",
    "        return loaders\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_all_datasets() -> Dict[str, Tuple[pd.DataFrame, pd.Series, str, str]]:\n",
    "        \"\"\"Load all available datasets with comprehensive display for educational purposes\"\"\"\n",
    "        \n",
    "        loaders = DatasetFactory.create_uci_loaders()\n",
    "        datasets = {}\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(\"LOADING AND OPTIMIZING DATASETS - EDUCATIONAL OVERVIEW\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"This section demonstrates the complete data loading and optimization process\")\n",
    "        print(\"for reproducibility and practitioner guidance.\\n\")\n",
    "        \n",
    "        logger.info(f\"Loading {len(loaders)} UCI datasets...\")\n",
    "        \n",
    "        for i, loader in enumerate(loaders, 1):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"DATASET {i}/{len(loaders)}: {loader.name.upper()}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(f\"Domain: {loader.domain}\")\n",
    "            print(f\"UCI ID: {loader.dataset_id}\")\n",
    "            \n",
    "            try:\n",
    "                # Load dataset with detailed reporting\n",
    "                X, y, description = loader.load()\n",
    "                \n",
    "                # Validate minimum size requirements for meaningful analysis\n",
    "                if len(X) >= 1000 and len(X.columns) >= 3:\n",
    "                    dataset_key = loader.name.lower().replace(\" \", \"_\")\n",
    "                    \n",
    "                    print(f\"\\n📊 DATASET OVERVIEW:\")\n",
    "                    print(f\"   Samples: {len(X):,}\")\n",
    "                    print(f\"   Features: {len(X.columns)}\")\n",
    "                    print(f\"   Target balance: {y.mean():.1%} positive class\")\n",
    "                    print(f\"   Memory usage: {X.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "                    \n",
    "                    # Display feature types breakdown\n",
    "                    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "                    categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "                    bool_cols = X.select_dtypes(include=['bool']).columns\n",
    "                    print(f\"   Feature types: {len(numeric_cols)} numeric, {len(categorical_cols)} categorical, {len(bool_cols)} boolean\")\n",
    "                    \n",
    "                    # Show missing data pattern\n",
    "                    missing_data = X.isnull().sum()\n",
    "                    missing_features = missing_data[missing_data > 0]\n",
    "                    if len(missing_features) > 0:\n",
    "                        print(f\"   Missing data: {len(missing_features)} features with missing values\")\n",
    "                        print(f\"   Total missing: {missing_data.sum():,} values ({missing_data.sum()/(len(X)*len(X.columns))*100:.1f}%)\")\n",
    "                    else:\n",
    "                        print(f\"   Missing data: Complete dataset (no missing values)\")\n",
    "                    \n",
    "                    print(f\"\\n🔍 FEATURE INSPECTION:\")\n",
    "                    print(f\"   First 5 feature names: {list(X.columns[:5])}\")\n",
    "                    if len(X.columns) > 5:\n",
    "                        print(f\"   Last 5 feature names: {list(X.columns[-5:])}\")\n",
    "                    \n",
    "                    # Display dataset head for educational purposes\n",
    "                    print(f\"\\n📋 DATASET HEAD (First 3 rows, up to 8 columns):\")\n",
    "                    display_cols = min(8, len(X.columns))\n",
    "                    head_display = X.iloc[:3, :display_cols].copy()\n",
    "                    \n",
    "                    # Format the display for readability\n",
    "                    for col in head_display.columns:\n",
    "                        if head_display[col].dtype == 'object':\n",
    "                            # Truncate long strings\n",
    "                            head_display[col] = head_display[col].astype(str).apply(\n",
    "                                lambda x: x[:15] + \"...\" if len(str(x)) > 15 else x\n",
    "                            )\n",
    "                        elif np.issubdtype(head_display[col].dtype, np.floating):\n",
    "                            # Round floats for readability\n",
    "                            head_display[col] = head_display[col].round(3)\n",
    "                    \n",
    "                    print(head_display.to_string())\n",
    "                    \n",
    "                    if len(X.columns) > display_cols:\n",
    "                        print(f\"   ... and {len(X.columns) - display_cols} more features\")\n",
    "                    \n",
    "                    print(f\"\\n🎯 TARGET VARIABLE SAMPLE:\")\n",
    "                    target_sample = y.head(10).tolist()\n",
    "                    print(f\"   First 10 values: {target_sample}\")\n",
    "                    print(f\"   Distribution: {sum(target_sample)}/{len(target_sample)} positive\")\n",
    "                    \n",
    "                    # Store the dataset\n",
    "                    datasets[dataset_key] = (X, y, description, loader.get_domain())\n",
    "                    \n",
    "                    print(f\"   ✅ {loader.name} successfully loaded and optimized\")\n",
    "                    \n",
    "                else:\n",
    "                    print(f\"   ❌ Dataset too small for meaningful analysis\")\n",
    "                    print(f\"      Samples: {len(X):,} (minimum: 1,000)\")\n",
    "                    print(f\"      Features: {len(X.columns)} (minimum: 3)\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Loading failed: {str(e)}\")\n",
    "                logger.error(f\"✗ {loader.name} failed to load: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Final summary\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"DATASET LOADING SUMMARY\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Successfully loaded: {len(datasets)}/{len(loaders)} datasets\")\n",
    "        \n",
    "        if datasets:\n",
    "            total_samples = sum(len(X) for X, y, desc, domain in datasets.values())\n",
    "            total_features = sum(len(X.columns) for X, y, desc, domain in datasets.values())\n",
    "            domains = set(domain for X, y, desc, domain in datasets.values())\n",
    "            \n",
    "            print(f\"Total samples: {total_samples:,}\")\n",
    "            print(f\"Total features: {total_features:,}\")\n",
    "            print(f\"Domains covered: {len(domains)}\")\n",
    "            print(f\"Domain list: {sorted(domains)}\")\n",
    "            \n",
    "            print(f\"\\n📚 LOADED DATASETS:\")\n",
    "            for name, (X, y, desc, domain) in datasets.items():\n",
    "                print(f\"   • {name}: {len(X):,} samples × {len(X.columns)} features ({domain})\")\n",
    "        else:\n",
    "            print(\"⚠️  No datasets successfully loaded\")\n",
    "            \n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        logger.info(f\"Successfully loaded {len(datasets)}/{len(loaders)} datasets\")\n",
    "        return datasets\n",
    "\n",
    "# Execute dataset loading\n",
    "if UCI_AVAILABLE:\n",
    "    print(\"Loading UCI datasets...\")\n",
    "    all_datasets = DatasetFactory.load_all_datasets()\n",
    "else:\n",
    "    print(\"UCI repository not available - creating empty dataset collection\")\n",
    "    all_datasets = {}\n",
    "\n",
    "print(f\"\\nDataset loading completed: {len(all_datasets)} datasets available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f42988da-13a2-4deb-8fd7-eff841db5721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FRAMEWORK STATUS: READY FOR EXECUTION\n",
      "================================================================================\n",
      "❌ Prerequisites not met\n",
      "  Missing: Sufficient datasets (have 0, need 3+)\n",
      "\n",
      "Framework is ready but cannot execute without prerequisites\n",
      "================================================================================\n",
      "NOTEBOOK READY - ALL COMPONENTS IMPLEMENTED\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 9: Execute Complete Study - Ready for Analysis\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FRAMEWORK STATUS: READY FOR EXECUTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if UCI_AVAILABLE and len(all_datasets) >= 3:\n",
    "    print(\"✅ All prerequisites met\")\n",
    "    print(f\"  UCI repository: Available\")\n",
    "    print(f\"  Datasets loaded: {len(all_datasets)}\")\n",
    "    print(f\"  Framework: Complete and tested\")\n",
    "    \n",
    "    print(f\"\\nLoaded datasets:\")\n",
    "    for name, (X, y, desc, domain) in all_datasets.items():\n",
    "        memory_mb = X.memory_usage(deep=True).sum() / 1024**2\n",
    "        bool_cols = len(X.select_dtypes(include=['bool']).columns)\n",
    "        print(f\"  {name}: {len(X):,} samples, {len(X.columns)} features, {memory_mb:.1f}MB\")\n",
    "        if bool_cols > 0:\n",
    "            print(f\"    → {bool_cols} boolean optimizations applied\")\n",
    "    \n",
    "    total_samples = sum(len(X) for X, y, _, _ in all_datasets.values())\n",
    "    print(f\"\\nTotal study scope: {total_samples:,} samples across {len(all_datasets)} domains\")\n",
    "    \n",
    "    print(f\"\\n🚀 READY TO EXECUTE COMPLETE STUDY\")\n",
    "    print(\"Next step: Execute the complete framework on all datasets\")\n",
    "    print(\"Estimated execution time: 30-60 minutes\")\n",
    "    print(\"Use: execute_publication_ready_study() when ready to proceed\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Prerequisites not met\")\n",
    "    if not UCI_AVAILABLE:\n",
    "        print(\"  Missing: UCI repository (install with: pip install ucimlrepo)\")\n",
    "    if len(all_datasets) < 3:\n",
    "        print(f\"  Missing: Sufficient datasets (have {len(all_datasets)}, need 3+)\")\n",
    "    \n",
    "    print(\"\\nFramework is ready but cannot execute without prerequisites\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"NOTEBOOK READY - ALL COMPONENTS IMPLEMENTED\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
